{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66458d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3be84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_y_train = pd.Series([1,0,0,1,1,1,0,0])  # Replace with actual data\n",
    "y_test = pd.Series([0,0,0,1,1,1,0,0])  # Replace with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12213872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_y_train.to_csv('sampled_ytrain.csv', index = False)\n",
    "y_test.to_csv('ytest.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002ba2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "\n",
    "# Function to calculate classification metrics\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "# Function to save classification results to Excel\n",
    "def save_classification_results_to_excel(model_name, sheet_name, results, confusion):\n",
    "    file_name = f'{model_name}_classification_results.xlsx'\n",
    "    try:\n",
    "        workbook = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=sheet_name)\n",
    "    \n",
    "    sheet = workbook[sheet_name]\n",
    "    sheet.append(['Metric', 'Value'])\n",
    "    \n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "    \n",
    "    sheet.append([])\n",
    "    sheet.append(['Confusion Matrix'])\n",
    "    confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "    header = [''] + confusion_df.columns.tolist()\n",
    "    sheet.append(header)\n",
    "    for idx, row in confusion_df.iterrows():\n",
    "        sheet.append([idx] + row.tolist())\n",
    "\n",
    "    workbook.save(file_name)\n",
    "    print(f'Classification results saved to {file_name}, sheet: {sheet_name}')\n",
    "\n",
    "# Main function to run classification metrics\n",
    "def run_classification_metrics(sampled_y_train, y_test):\n",
    "    model_name_widget = widgets.Text(description='Model Name:')\n",
    "    dataset_type_widget = widgets.Dropdown(\n",
    "        options=[('Select One', 'select_one'), ('Train', 'train'), ('Test', 'test')],\n",
    "        description='Dataset Type:',\n",
    "    )\n",
    "    file_selector = widgets.FileUpload(description='Select y_pred File')\n",
    "    threshold_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='Threshold:')\n",
    "    run_button = widgets.Button(description='Run Metrics')\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_dataset_type_change(change):\n",
    "        clear_output(wait=True)\n",
    "        display(model_name_widget, dataset_type_widget)\n",
    "        if dataset_type_widget.value in ['train', 'test']:\n",
    "            display(file_selector, threshold_slider, run_button, output)\n",
    "    \n",
    "    def on_run_button_clicked(b):\n",
    "        with output:\n",
    "            model_name = model_name_widget.value.strip()\n",
    "            dataset_type = dataset_type_widget.value\n",
    "            threshold = threshold_slider.value\n",
    "            \n",
    "            if not model_name or dataset_type == 'select_one' or not file_selector.value:\n",
    "                print(\"Please enter a model name, select a dataset, and upload a file.\")\n",
    "                return\n",
    "            \n",
    "            uploaded_file = next(iter(file_selector.value.values()))['content']\n",
    "            y_pred = pd.read_csv(pd.io.common.BytesIO(uploaded_file)).iloc[:, 0]\n",
    "            y_actual = sampled_y_train if dataset_type == 'train' else y_test\n",
    "            y_pred = (y_pred >= threshold).astype(int)\n",
    "            \n",
    "            results, confusion = calculate_classification_metrics(y_actual, y_pred)\n",
    "            \n",
    "            display(widgets.HTML(f'<h3>Model: {model_name}, Dataset: {dataset_type}, Threshold: {threshold:.2f}</h3>'))\n",
    "            display(pd.DataFrame(results))\n",
    "            display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "            \n",
    "            save_classification_results_to_excel(model_name, os.path.basename(list(file_selector.value.keys())[0]), results, confusion)\n",
    "    \n",
    "    dataset_type_widget.observe(on_dataset_type_change, names='value')\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    display(model_name_widget, dataset_type_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5090981a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039f3b1239b3454eb73db97c466168f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='svm', description='Model Name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143dfc83eef94855a4e399760a572ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset Type:', index=2, options=(('Select One', 'select_one'), ('Train', 'train'), ('Te…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3871a9411041869b4b76f443495c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={'train_file.csv': {'metadata': {'name': 'train_file.csv', 'type': 'text/csv', 'size': 50, 'l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9c183bc17d4e22a485733850dcb146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.49, description='Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8906e1b136ba41c6b6fb1878b54631e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Metrics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b815143b9764465839466f9c42f87a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': \"HTML(value='<h3>Model: dtree, Dataset:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_classification_metrics(sampled_y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d89275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "\n",
    "# Function to calculate classification metrics\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "# Function to save classification results to Excel\n",
    "def save_classification_results_to_excel(directory, model_name, sheet_name, results, confusion):\n",
    "    file_name = os.path.join(directory, f'{model_name}_classification_results.xlsx')\n",
    "    try:\n",
    "        workbook = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=sheet_name)\n",
    "    \n",
    "    sheet = workbook[sheet_name]\n",
    "    sheet.append(['Metric', 'Value'])\n",
    "    \n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "    \n",
    "    sheet.append([])\n",
    "    sheet.append(['Confusion Matrix'])\n",
    "    confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "    header = [''] + confusion_df.columns.tolist()\n",
    "    sheet.append(header)\n",
    "    for idx, row in confusion_df.iterrows():\n",
    "        sheet.append([idx] + row.tolist())\n",
    "\n",
    "    workbook.save(file_name)\n",
    "    print(f'Classification results saved to {file_name}, sheet: {sheet_name}')\n",
    "\n",
    "# Main function to run classification metrics\n",
    "def run_classification_metrics():\n",
    "    directory_selector = widgets.Text(description='Directory Path:')\n",
    "    dir_confirm_button = widgets.Button(description='Confirm Directory')\n",
    "    file_selector_train = widgets.Dropdown(description='Select sampled_y_train')\n",
    "    file_selector_test = widgets.Dropdown(description='Select y_test')\n",
    "    confirm_files_button = widgets.Button(description='Confirm Files')\n",
    "    \n",
    "    model_name_widget = widgets.Text(description='Model Name:')\n",
    "    dataset_type_widget = widgets.Dropdown(\n",
    "        options=[('Select One', 'select_one'), ('Train', 'train'), ('Test', 'test')],\n",
    "        description='Dataset Type:',\n",
    "    )\n",
    "    file_selector_pred = widgets.FileUpload(description='Select y_pred File')\n",
    "    threshold_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='Threshold:')\n",
    "    run_button = widgets.Button(description='Run Metrics')\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    selected_directory = {'path': None}\n",
    "    selected_files = {'train': None, 'test': None}\n",
    "    \n",
    "    def update_file_selectors():\n",
    "        if selected_directory['path']:\n",
    "            files = [f for f in os.listdir(selected_directory['path']) if f.endswith('.csv')]\n",
    "            file_selector_train.options = files\n",
    "            file_selector_test.options = files\n",
    "            file_selector_train.value = files[0] if files else None\n",
    "            file_selector_test.value = files[0] if files else None\n",
    "    \n",
    "    def on_dir_confirm_clicked(b):\n",
    "        selected_directory['path'] = directory_selector.value.strip()\n",
    "        if os.path.isdir(selected_directory['path']):\n",
    "            update_file_selectors()\n",
    "            display(file_selector_train, file_selector_test, confirm_files_button)\n",
    "        else:\n",
    "            print(\"Invalid directory. Please enter a valid path.\")\n",
    "    \n",
    "    def on_confirm_files_clicked(b):\n",
    "        selected_files['train'] = os.path.join(selected_directory['path'], file_selector_train.value)\n",
    "        selected_files['test'] = os.path.join(selected_directory['path'], file_selector_test.value)\n",
    "        display(model_name_widget, dataset_type_widget)\n",
    "    \n",
    "    def on_dataset_type_change(change):\n",
    "        clear_output(wait=True)\n",
    "        display(directory_selector, dir_confirm_button, file_selector_train, file_selector_test, confirm_files_button, model_name_widget, dataset_type_widget)\n",
    "        if dataset_type_widget.value in ['train', 'test']:\n",
    "            display(file_selector_pred, threshold_slider, run_button, output)\n",
    "    \n",
    "    def on_run_button_clicked(b):\n",
    "        with output:\n",
    "            model_name = model_name_widget.value.strip()\n",
    "            dataset_type = dataset_type_widget.value\n",
    "            threshold = threshold_slider.value\n",
    "            \n",
    "            if not model_name or dataset_type == 'select_one' or not file_selector_pred.value:\n",
    "                print(\"Please enter a model name, select a dataset, and upload a file.\")\n",
    "                return\n",
    "            \n",
    "            uploaded_file = next(iter(file_selector_pred.value.values()))['content']\n",
    "            y_pred = pd.read_csv(pd.io.common.BytesIO(uploaded_file)).iloc[:, 0]\n",
    "            y_actual_file = selected_files['train'] if dataset_type == 'train' else selected_files['test']\n",
    "            y_actual = pd.read_csv(y_actual_file).iloc[:, 0]\n",
    "            y_pred = (y_pred >= threshold).astype(int)\n",
    "            \n",
    "            results, confusion = calculate_classification_metrics(y_actual, y_pred)\n",
    "            \n",
    "            display(widgets.HTML(f'<h3>Model: {model_name}, Dataset: {dataset_type}, Threshold: {threshold:.2f}</h3>'))\n",
    "            display(pd.DataFrame(results))\n",
    "            display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "            \n",
    "            save_classification_results_to_excel(selected_directory['path'], model_name, os.path.basename(list(file_selector_pred.value.keys())[0]), results, confusion)\n",
    "    \n",
    "    dir_confirm_button.on_click(on_dir_confirm_clicked)\n",
    "    confirm_files_button.on_click(on_confirm_files_clicked)\n",
    "    dataset_type_widget.observe(on_dataset_type_change, names='value')\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    display(directory_selector, dir_confirm_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b73549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d571f9ad869742cfaa8c8dc33c31e95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='C:\\\\Users\\\\adity\\\\Downloads\\\\aditya_automl_code\\\\testing', description='Directory Path:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d347cce746a4126b1e2e6c873de1bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Directory', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d115300e5da44b5b985b6d1b2b4057b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select sampled_y_train', options=('sampled_ytrain.csv', 'test_file.csv', 'train_file.csv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033d3eb80ff743cdb4101c262756c5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select y_test', index=3, options=('sampled_ytrain.csv', 'test_file.csv', 'train_file.csv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19171871e41840799441b1fb1278917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Files', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8b73adfea24fba9a15ca7eda23a3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='tt', description='Model Name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a301d17c0441efa4677a0579d1e4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset Type:', index=1, options=(('Select One', 'select_one'), ('Train', 'train'), ('Te…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3c10fc0b2541fa98196457ddd22094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Select y_pred File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcc7ec0130f465881146215144b7d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8a008ecca64697be1b6e52fba5bba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Metrics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e4bb13d5b24f60aa716c32764ca85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_classification_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015d6d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-89c614a32bab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_classification_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sampled_y_train' is not defined"
     ]
    }
   ],
   "source": [
    "run_classification_metrics(sampled_y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f181b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
