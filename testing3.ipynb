{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66458d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3be84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_y_train = pd.Series([1,0,0,1,1,1,0,0])  # Replace with actual data\n",
    "y_test = pd.Series([0,0,0,1,1,1,0,0])  # Replace with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002ba2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "\n",
    "# Function to calculate classification metrics\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "# Function to save classification results to Excel\n",
    "def save_classification_results_to_excel(model_name, sheet_name, results, confusion):\n",
    "    file_name = f'{model_name}_classification_results.xlsx'\n",
    "    try:\n",
    "        workbook = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=sheet_name)\n",
    "    \n",
    "    sheet = workbook[sheet_name]\n",
    "    sheet.append(['Metric', 'Value'])\n",
    "    \n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "    \n",
    "    sheet.append([])\n",
    "    sheet.append(['Confusion Matrix'])\n",
    "    confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "    header = [''] + confusion_df.columns.tolist()\n",
    "    sheet.append(header)\n",
    "    for idx, row in confusion_df.iterrows():\n",
    "        sheet.append([idx] + row.tolist())\n",
    "\n",
    "    workbook.save(file_name)\n",
    "    print(f'Classification results saved to {file_name}, sheet: {sheet_name}')\n",
    "\n",
    "# Main function to run classification metrics\n",
    "def run_classification_metrics(sampled_y_train, y_test):\n",
    "    model_name_widget = widgets.Text(description='Model Name:')\n",
    "    dataset_type_widget = widgets.Dropdown(\n",
    "        options=[('Select One', 'select_one'), ('Train', 'train'), ('Test', 'test')],\n",
    "        description='Dataset Type:',\n",
    "    )\n",
    "    file_selector = widgets.FileUpload(description='Select y_pred File')\n",
    "    threshold_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='Threshold:')\n",
    "    run_button = widgets.Button(description='Run Metrics')\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_dataset_type_change(change):\n",
    "        clear_output(wait=True)\n",
    "        display(model_name_widget, dataset_type_widget)\n",
    "        if dataset_type_widget.value in ['train', 'test']:\n",
    "            display(file_selector, threshold_slider, run_button, output)\n",
    "    \n",
    "    def on_run_button_clicked(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            model_name = model_name_widget.value.strip()\n",
    "            dataset_type = dataset_type_widget.value\n",
    "            threshold = threshold_slider.value\n",
    "            \n",
    "            if not model_name or dataset_type == 'select_one' or not file_selector.value:\n",
    "                print(\"Please enter a model name, select a dataset, and upload a file.\")\n",
    "                return\n",
    "            \n",
    "            uploaded_file = next(iter(file_selector.value.values()))['content']\n",
    "            y_pred = pd.read_csv(pd.io.common.BytesIO(uploaded_file)).iloc[:, 0]\n",
    "            y_actual = sampled_y_train if dataset_type == 'train' else y_test\n",
    "            y_pred = (y_pred >= threshold).astype(int)\n",
    "            \n",
    "            results, confusion = calculate_classification_metrics(y_actual, y_pred)\n",
    "            display(pd.DataFrame(results))\n",
    "            display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "            \n",
    "            save_classification_results_to_excel(model_name, os.path.basename(list(file_selector.value.keys())[0]), results, confusion)\n",
    "    \n",
    "    dataset_type_widget.observe(on_dataset_type_change, names='value')\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    display(model_name_widget, dataset_type_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5090981a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebf72d4b0964bc6921c23ff935ae77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='dtree', description='Model Name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe2348bc7c24fb39fd5f1cd3258773d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset Type:', index=2, options=(('Select One', 'select_one'), ('Train', 'train'), ('Te…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6ce4c3fe304e4fa8e8b89fdfa74b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={'train_file.csv': {'metadata': {'name': 'train_file.csv', 'type': 'text/csv', 'size': 50, 'l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad333275dff48e190193b1c95811937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaa7cdcde834685ab461f2097fcf2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Metrics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc9798494944d5a9f169e4a0e92ef79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '          Metric     Value\\n0      Pre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_classification_metrics(sampled_y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
