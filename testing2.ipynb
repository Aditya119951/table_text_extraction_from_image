{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9e9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "\n",
    "# Function to calculate classification metrics\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "\n",
    "    return metrics, confusion\n",
    "\n",
    "# Function to save classification results to Excel\n",
    "def save_classification_results_to_excel(model_name, file_name, results, confusion):\n",
    "    excel_file = f\"{model_name}_classification_results.xlsx\"\n",
    "    try:\n",
    "        workbook = load_workbook(excel_file)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if file_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=file_name)\n",
    "    \n",
    "    sheet = workbook[file_name]\n",
    "    \n",
    "    if sheet.max_row == 1:\n",
    "        sheet.append(['Metric', 'Value'])\n",
    "    \n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "\n",
    "    if len(confusion) > 0:\n",
    "        sheet.append([])\n",
    "        sheet.append(['Confusion Matrix'])\n",
    "        \n",
    "        confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "        header = [''] + confusion_df.columns.tolist()\n",
    "        sheet.append(header)\n",
    "        \n",
    "        for idx, row in confusion_df.iterrows():\n",
    "            sheet.append([idx] + row.tolist())\n",
    "    \n",
    "    workbook.save(excel_file)\n",
    "    print(f'Classification results saved to {excel_file}, sheet: {file_name}')\n",
    "\n",
    "# Main function\n",
    "\n",
    "def run_classification_metrics(sampled_y_train, y_test):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Model selection\n",
    "    model_name_dropdown = widgets.Text(description=\"Model Name:\")\n",
    "    confirm_model_button = widgets.Button(description=\"Confirm Model\")\n",
    "    \n",
    "    def on_model_confirmed(b):\n",
    "        model_name = model_name_dropdown.value.strip()\n",
    "        if model_name:\n",
    "            clear_output(wait=True)\n",
    "            display(dataset_type)\n",
    "            dataset_type.observe(lambda change: on_dataset_selected(change, model_name), names='value')\n",
    "    \n",
    "    confirm_model_button.on_click(on_model_confirmed)\n",
    "    display(model_name_dropdown, confirm_model_button)\n",
    "    \n",
    "    dataset_type = widgets.Dropdown(\n",
    "        options=[('Select One', 'select_one'), ('Train', 'train'), ('Test', 'test')],\n",
    "        description='Dataset Type:',\n",
    "    )\n",
    "    \n",
    "    def on_dataset_selected(change, model_name):\n",
    "        clear_output(wait=True)\n",
    "        display(dataset_type)\n",
    "        \n",
    "        if change['new'] == 'train':\n",
    "            y_actual = sampled_y_train\n",
    "            file_selector = widgets.FileUpload(accept=\".csv\", multiple=False)\n",
    "            confirm_file_button = widgets.Button(description=\"Confirm File\")\n",
    "            \n",
    "            def on_file_selected(b):\n",
    "                for uploaded_file in file_selector.value:\n",
    "                    file_name = uploaded_file\n",
    "                    content = file_selector.value[file_name]['content']\n",
    "                    y_pred = pd.read_csv(pd.io.common.BytesIO(content)).iloc[:, 0]\n",
    "                    compute_metrics(y_actual, y_pred, model_name, file_name)\n",
    "                    return\n",
    "            \n",
    "            confirm_file_button.on_click(on_file_selected)\n",
    "            display(file_selector, confirm_file_button)\n",
    "        elif change['new'] == 'test':\n",
    "            y_actual = y_test\n",
    "            threshold_slider.value = 0.5\n",
    "            compute_metrics(y_actual, y_test, model_name, \"Test Data\")\n",
    "\n",
    "    threshold_slider = widgets.FloatSlider(\n",
    "        value=0.5, min=0.0, max=1.0, step=0.01, description='Threshold:',\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(y_actual, y_pred, model_name, file_name):\n",
    "        threshold = threshold_slider.value\n",
    "        y_pred = (y_pred >= threshold).astype(int)\n",
    "        results, confusion = calculate_classification_metrics(y_actual, y_pred)\n",
    "        display(widgets.HTML(f'<h4>Model Metrics (Threshold = {threshold:.2f})</h4>'))\n",
    "        display(pd.DataFrame(results))\n",
    "        display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "        save_classification_results_to_excel(model_name, file_name, results, confusion)\n",
    "    \n",
    "    display(threshold_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bbdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567f995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tkinter import Tk, filedialog\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "def save_classification_results_to_excel(model_name, results, confusion, dataset_type, sheet_name):\n",
    "    file_name = f'{model_name}_classification_results.xlsx'\n",
    "    try:\n",
    "        workbook = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=sheet_name)\n",
    "    \n",
    "    sheet = workbook[sheet_name]\n",
    "    if sheet.max_row == 1:\n",
    "        sheet.append(['Metric', 'Value'])\n",
    "\n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "\n",
    "    if len(confusion) > 0:\n",
    "        sheet.append([])\n",
    "        sheet.append(['Confusion Matrix'])\n",
    "        confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "        header = [''] + confusion_df.columns.tolist()\n",
    "        sheet.append(header)\n",
    "        for idx, row in confusion_df.iterrows():\n",
    "            sheet.append([idx] + row.tolist())\n",
    "\n",
    "    workbook.save(file_name)\n",
    "    print(f'Classification results saved to {file_name}, sheet: {sheet_name}')\n",
    "\n",
    "def select_file():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(title=\"Select the file for y_pred\")\n",
    "    return file_path\n",
    "\n",
    "def run_classification_metrics(sampled_y_train, y_test):\n",
    "    model_name = widgets.Text(description=\"Model Name:\")\n",
    "    dataset_type = widgets.Dropdown(\n",
    "        options=[('Select One', 'select_one'), ('Train', 'train'), ('Test', 'test')],\n",
    "        description='Dataset:'\n",
    "    )\n",
    "    threshold_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='Threshold:')\n",
    "    run_button = widgets.Button(description='Run Metrics')\n",
    "\n",
    "    def on_run_button_clicked(b):\n",
    "        clear_output(wait=True)\n",
    "        display(model_name, dataset_type, threshold_slider, run_button)\n",
    "        \n",
    "        if dataset_type.value == 'select_one':\n",
    "            print(\"Please select a valid dataset type.\")\n",
    "            return\n",
    "\n",
    "        file_path = select_file()\n",
    "        if not file_path:\n",
    "            print(\"No file selected.\")\n",
    "            return\n",
    "        \n",
    "        y_pred = pd.read_csv(file_path).iloc[:, 0]\n",
    "        y_actual = sampled_y_train if dataset_type.value == 'train' else y_test\n",
    "        threshold = threshold_slider.value\n",
    "        y_pred = (y_pred >= threshold).astype(int)\n",
    "        \n",
    "        results, confusion = calculate_classification_metrics(y_actual, y_pred)\n",
    "        sheet_name = file_path.split('/')[-1].split('.')[0]\n",
    "        save_classification_results_to_excel(model_name.value, results, confusion, dataset_type.value, sheet_name)\n",
    "        \n",
    "        display(pd.DataFrame(results))\n",
    "        display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "    \n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    display(model_name, dataset_type, threshold_slider, run_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad883169",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5263e41c491a485ea3eb96bb234664f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='dtree', description='Model Name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa2e0c258a74733af77a8981c08c11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', index=2, options=(('Select One', 'select_one'), ('Train', 'train'), ('Test', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee3033be7b341339c579bb547a1cd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Threshold:', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f139442aca47289f52b701bf7817f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Metrics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results saved to dtree_classification_results.xlsx, sheet: test_file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUC ROC Score</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GINI</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric  Value\n",
       "0      Precision    0.0\n",
       "1         Recall    0.0\n",
       "2       F1-Score    0.0\n",
       "3  AUC ROC Score    0.1\n",
       "4           GINI   -0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0            1            4\n",
       "Actual 1            3            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_classification_metrics(sampled_y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa251fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aaf85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    gini = 2 * auc - 1\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score', 'AUC ROC Score', 'GINI'],\n",
    "        'Value': [precision, recall, f1, auc, gini]\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "def save_classification_results_to_excel(results, confusion, model_name, dataset_type, sheet_name):\n",
    "    file_name = f'{model_name}.xlsx'\n",
    "    try:\n",
    "        workbook = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        if 'Sheet' in workbook.sheetnames:\n",
    "            del workbook['Sheet']\n",
    "\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        workbook.create_sheet(title=sheet_name)\n",
    "    \n",
    "    sheet = workbook[sheet_name]\n",
    "    \n",
    "    sheet.append(['Metric', 'Value'])\n",
    "    for metric, value in zip(results['Metric'], results['Value']):\n",
    "        sheet.append([metric, value])\n",
    "    \n",
    "    sheet.append([])\n",
    "    sheet.append(['Confusion Matrix'])\n",
    "    confusion_df = pd.DataFrame(confusion, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "    for idx, row in confusion_df.iterrows():\n",
    "        sheet.append([idx] + row.tolist())\n",
    "    \n",
    "    workbook.save(file_name)\n",
    "    print(f'Results saved to {file_name} under sheet: {sheet_name}')\n",
    "\n",
    "def create_widgets_for_metrics(prediction_folder, sampled_y_train, y_test):\n",
    "    model_names = [\"Decision Tree\", \"Random Forest\", \"LightGBM\", \"XGBoost\"]  # Modify as needed\n",
    "    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=model_names,\n",
    "        description='Model:',\n",
    "    )\n",
    "    \n",
    "    def on_model_selected(change):\n",
    "        clear_output(wait=True)\n",
    "        display(model_dropdown)\n",
    "        selected_model = model_dropdown.value\n",
    "        print(f'Selected Model: {selected_model}')\n",
    "        \n",
    "        dataset_type = widgets.Dropdown(\n",
    "            options=[('Train', 'train'), ('Test', 'test')],\n",
    "            description='Dataset:',\n",
    "        )\n",
    "        \n",
    "        file_selector = widgets.Dropdown(\n",
    "            options=[f for f in os.listdir(prediction_folder) if f.endswith('.csv')],\n",
    "            description='Pred File:',\n",
    "        )\n",
    "        \n",
    "        def generate_report(b):\n",
    "            selected_dataset = dataset_type.value\n",
    "            selected_file = file_selector.value\n",
    "            \n",
    "            if selected_dataset == 'train':\n",
    "                y_true = sampled_y_train\n",
    "            elif selected_dataset == 'test':\n",
    "                y_true = y_test\n",
    "            else:\n",
    "                print(\"Please select a valid dataset.\")\n",
    "                return\n",
    "            \n",
    "            y_pred_path = os.path.join(prediction_folder, selected_file)\n",
    "            y_pred = pd.read_csv(y_pred_path).iloc[:, 0]  # Assuming first column has predictions\n",
    "            \n",
    "            results, confusion = calculate_classification_metrics(y_true, y_pred)\n",
    "            \n",
    "            display(pd.DataFrame(results))\n",
    "            display(pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "            \n",
    "            save_classification_results_to_excel(results, confusion, selected_model, selected_dataset, selected_file.replace('.csv', ''))\n",
    "        \n",
    "        generate_button = widgets.Button(description='Generate Report')\n",
    "        generate_button.on_click(generate_report)\n",
    "        \n",
    "        display(dataset_type, file_selector, generate_button)\n",
    "    \n",
    "    model_dropdown.observe(on_model_selected, names='value')\n",
    "    display(model_dropdown)\n",
    "\n",
    "# Example usage (update the paths and data accordingly)\n",
    "prediction_folder = \"C:/Users/adity/Downloads/aditya_automl_code/testing/\"\n",
    "sampled_y_train = pd.Series([1,0,0,1,1,1,0,0])  # Replace with actual data\n",
    "y_test = pd.Series([0,0,0,1,1,1,0,0])  # Replace with actual data\n",
    "create_widgets_for_metrics(prediction_folder, sampled_y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35582f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1819ed14b8fe4cd5ba667134c1dc4011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('test_file',), value='test_file')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91be1d20661e46558971067bfdf617b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', options=(('Train', 'train'), ('Test', 'test')), value='train')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff75133723840808e438bf3f6de3a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Pred File:', options=('test_file.csv',), value='test_file.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04364a0c559542d8a75077903a42fb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Report', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7de44ab820a>\u001b[0m in \u001b[0;36mgenerate_report\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Assuming the first column contains predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_classification_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-b7de44ab820a>\u001b[0m in \u001b[0;36mcalculate_classification_metrics\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_classification_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m     \"\"\"\n\u001b[1;32m-> 2131\u001b[1;33m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   2132\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"skip_parameter_validation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1722\u001b[0m     \"\"\"\n\u001b[0;32m   1723\u001b[0m     \u001b[0mzero_division_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_zero_division\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"average has to be one of \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m     \u001b[1;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m     \u001b[1;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8, 5]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2479def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65efc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_y_train = pd.Series([1,0,0,1,1,1,0,0])  # Replace with actual data\n",
    "y_test = pd.Series([0,0,0,1,1,1,0,0])  # Replace with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d94da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'a':[0.99,0.89,0.67,0.33,0.21,0.99,0.88,0.5]}).to_csv('train_file.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949ab398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'a':[0.91,0.84,0.66,0.22,0.11,0.19,0.44,0.53]}).to_csv('test_file.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0728aa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  1\n",
       "1  2\n",
       "2  3\n",
       "3  4\n",
       "4  5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('C:/Users/adity/Downloads/aditya_automl_code/testing/test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f541c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
